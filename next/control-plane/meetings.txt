~silsyn-wathep

Attempt to braindump the current state of the `%khan` control plane.

## PRs

* C driver -> next/vere:
  <https://github.com/urbit/urbit/pull/5345>
* Haskell client -> master:
  <https://github.com/urbit/urbit/pull/5385>
* Vane -> next/arvo:
  <https://github.com/urbit/urbit/pull/5420>
* Obsolete vane + haskell driver combined PR:
  <https://github.com/urbit/urbit/pull/5351>

## Initial target

Initially it's +code, +hood/cors-approve 'https://horizon.tlon.network',
+hood/unlink %chat-cli, and whatever "send an invite to a group from ship a ->
ship b" looks like

## Status

* +code get/reset is fully implemented end-to-end between the C king and
  Haskell client.
* |mass stub exists.
* The protocol is just command/response: every command receives either an %ack
  or a command-specific response whose head is the same @tas as the command
  itself.
* Any other message can come over the socket at any time, including %bail
  messages that the driver tries to send in the case of errors. (This is to
  support metric monitoring in future.)
* Aspirationally, the client and driver should immediately close the connection
  on any malformed message.

## Questions and theories

* What functionality do we want the vane to expose? Most restrictive: fixed set
  of commands / queries. Most permissive: arbitrary scry + arbitrary event
  injection. Josh suggested: arbitrary scry + arbitrary move.
* It is not currently possible to differentiate arbitrary newt protocol
  messages from ASCII text. We might want to preserve the ability to do so. How
  should we do this?
  * Proposal: change the framing so that the first 32 bits are always
    0x80 0x00 0x00 0x00 and the second 32-bit word is LSB-encoded length.
    (This reserves the first word for version / flag info in future; probably
    generally good practice.)
* How do we handle runtime queries? One option: reserve the %king cell head.

## TODOs/blockers

* Overall
  * Clean up error handling
  * Clean up protocol spec
  * Make a decision re newt framing
* C driver
  * Don't break Windows
  * Implement (or at least make room for) runtime-specific queries

## Workflow

Since this is targeting multiple different parts of the system at once, each
with their own release branches, I have been making heavy use of
`git worktree` for development. E.g. from a source urbit repo:

    git worktree add ../c3 jo/khan-c3
    git worktree add ../arvo jo/khan-arvo
    git worktree add ../into jo/into

Then build the king out of c3, use it to boot a fakezod, copy in and commit the
`pkg/arvo` directory from the `arvo` worktree (must be done manually since `-A`
does not work for me), and send commands with e.g. `stack run into ~/zod code`
from the `hs/` directory of the `into` worktree.

# Meetings

## ~2021.11.17

* ~master-morzod: what do we ship? what's a deliverable? for arbitrary scries /
  events, we don't need a vane.
* ~wicdev-wisryt: have a "hosting tools" desk, have threads in it, install them
  where you want to use them. "Inject event with the thread, then run the
  thread" could work for this. You need a vane for running threads with
  req/res. (spider is an app: generate unique id, subscribe, poke, wait.
  there's a pattern that works 100% of the time but you don't want to write it
  in haskell or C. or hoon.)
  * if you have a vane, say "invoke thread with arg", it gives you an answer;
    you can use that from outside, makes azimuth app simpler, makes anything
    that calls threads simpler
  * take an app that runs threads now, put that in a vane. (eth-watcher best.)
  * can support scries/pokes in driver directly
* ~master-morzod: hesitation abt expanding use of threads; it's theoretical,
  maybe not worth getting into. interesting possibility in placing this into a
  vane: paves way for kicking computations out of the loop. threads are
  suspendable, they already block. could imagine a thread interface that pushes
  computations out of the loop.
* where do things go?
  * ~master-morzod: if you implement in driver, you have to decide whether the
    paths are complete. vere can say "do it now" and push interpolation out to
    the worker.
* we need commands to the runtime, |meld etc
* monitoring, event frequency, notifications
* ~wicdev-wisryt: should scry, if ship and/or date are null, interpolate them
* TODO insert notes

* ~wicdev-wisryt post-meeting notes:
  - scry should interpolate ship and date if they're null. This should be done
    in arvo, so the driver is agnostic.
  - For reading runtime info, we should just overlay the namespace with
    runtime-specific stuff. Shouldn't try to standardize this until necessary.
  - Similarly, arbitrary moves can control the runtime. If the move is
    [%mars *], we handle it directly in the driver. Same about standardizing
  - Other subscriptions coming from userspace can be added to khan later,
    probably as "start a gall subscription, report all %facts, restart on
    kick". Details will change with subscription reform But none of that needs
    to happen now. The trio of scry, move, thread is plenty for a long time.

  Lmk if you need help on running a thread. It's delicate, but it would take me
  like an hour to write it. To understand it, read in app/eth-watcher the
  start-cards section of +on-arvo and the non-helper-function part of +on-agent

* ~widced-wisryt thread API

```
state:
stateless.  If later we want to support cancelling threads, add (map duct tid:spider)

duct from runtime:
//khan/0v1234/connection-number

task:
[%run-thread-v0 =beak name=term data=*]
[%run-thread-v1 =beak name=term =mark =cage]
::  XX run older revisions?
[%run-thread-v2 $@(=desk [=desk =case]) name=term =out=mark data=(cask)]
::  when invoking, cask => cage
::  you can scry for a conversion gate.
::  look up the mark, convert to a vase.

gift:
[%thread-result (each cage [term tang])]
[%thread-result-v2 (each (cask) goof]

basic flow:
- watch on /thread-result/[thread-id]
- poke with %spider-start
- receive poke-ack, complain if failed
- receive watch-ack, complain if failed
- receive %fact with mark %thread-fail or %thread-done
- receive %kick

++  poke-spider
  |=  [our=@p =cage]
  [%pass /start-thread %g %deal [our our] %spider %poke cage]
```

## ~2022.1.4

After-infra meeting notes:
- %urth in the peek $% is weird; if we're not going to make it a deeper fiction
  we should move it out to top level
- making request-id a general atom and storing it would allow responses across
  connections
- move: idea was it's an arbitrary task for kernel, would not go to khan.
- injecting ova: invariants must be maintained. you have raw access to any
  driver. if you want effects back here, need a wire.
- ted: you're asking arvo to do something. that's an event arvo will send to
  khan as a task. can be completely generic; khan can ask arvo, other vanes.
- no question: you need arbitrary event injection. question is: do we want a
  happy path?
- normally wrap things with khan, have escape to arbitary event injection
- is there a response type?
- request ids: stateless
- peeks currently just close the socket on error and print the message out to
  the console. if we want error codes, we want state; if we want state, we may
  not want client-controlled request ids.

## ~2022.1.6

- we indeed don't want to do a namespace overlay for the runtime.
- the interface desired is:
  - start a thread
  - scry
  - inject arbitrary ovum
  - runtime peek
  - runtime command
- it may be expedient to combine the runtime peeks and commands into one arm.
- it may be expedient to have the khan vane implement threads instead of spider.
- we probably want to decouple the vane from the driver, naming the driver
  something like `conn`.
